<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Distributed Classes · HealpixMPI.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="HealpixMPI.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">HealpixMPI.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Introduction</a></li><li class="is-active"><a class="tocitem" href>Distributed Classes</a><ul class="internal"><li><a class="tocitem" href="#Initializing-a-Distributed-type"><span>Initializing a Distributed type</span></a></li><li><a class="tocitem" href="#Gathering-data"><span>Gathering data</span></a></li><li><a class="tocitem" href="#Distributing-Strategy"><span>Distributing Strategy</span></a></li></ul></li><li><a class="tocitem" href="../sht/">Spherical Harmonics</a></li><li><a class="tocitem" href="../misc/">Miscellanea</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Distributed Classes</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Distributed Classes</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/main/docs/src/distribute.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Distributed-Classes"><a class="docs-heading-anchor" href="#Distributed-Classes">Distributed Classes</a><a id="Distributed-Classes-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Classes" title="Permalink"></a></h1><p>As mentioned in the introduction, HealpixMPI has the main purpose of providing an MPI parallelization of the main functionalities of <a href="https://github.com/ziotom78/Healpix.jl">Healpix.jl</a>, distributing maps and harmonic coefficients over the MPI tasks efficiently. This is made possible by the implementation of two data types: <a href="#HealpixMPI.DMap"><code>DMap</code></a> and <a href="#HealpixMPI.DAlm"><code>DAlm</code></a>, mirroring <a href="https://ziotom78.github.io/Healpix.jl/stable/mapfunc/#Healpix.HealpixMap"><code>HealpixMap</code></a> and <a href="https://ziotom78.github.io/Healpix.jl/stable/alm/#Healpix.Alm"><code>Alm</code></a> types of Healpix.jl respectively, and containing a well-defined subset of a map or harmonic coefficients, to be constructed on each MPI task.</p><article class="docstring"><header><a class="docstring-binding" id="HealpixMPI.DMap" href="#HealpixMPI.DMap"><code>HealpixMPI.DMap</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct DMap{T&lt;:Number, I&lt;:Integer}</code></pre><p>A subset of a Healpix map, containing only certain rings (as specified in the <code>info</code> field). The type <code>T</code> is used for the value of the pixels in a map, it must be a <code>Number</code> (usually float).</p><p>A <code>DMap</code> type contains the following fields:</p><ul><li><code>pixels::Vector{T}</code>: array of pixels composing the subset.</li><li><code>info::GeomInfo</code>: a <code>GeomInfo</code> object describing the HealpixMap subset.</li></ul><p>The <code>GeomInfoMPI</code> contained in <code>info</code> must match exactly the characteristic of the Map subset, this is already automatically constructed when <a href="#MPI.Scatter!"><code>MPI.Scatter!</code></a> is called, reason why this method for initializing a <code>DMap</code> is reccomended.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/9d8251dccbfca45ddb28bf0ee38c1c01efe53410/src/map.jl#L48-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="HealpixMPI.DAlm" href="#HealpixMPI.DAlm"><code>HealpixMPI.DAlm</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct DAlm{S&lt;:Strategy, T&lt;:Number, I&lt;:Integer}</code></pre><p>An MPI-distributed subset of harmonic coefficients a_ℓm, referring only to certain values of m.</p><p>The type <code>T</code> is used for the value of each harmonic coefficient, and it must be a <code>Number</code> (one should however only use complex types for this).</p><p>A <code>SubAlm</code> type contains the following fields:</p><ul><li><code>alm</code>: the array of harmonic coefficients</li><li><code>info</code>: an <code>AlmInfo</code> object describing the alm subset.</li></ul><p>The <code>AlmInfo</code> contained in <code>info</code> must match exactly the characteristic of the Alm subset, this can be constructed through the function <code>make_general_alm_info</code>, for instance.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/9d8251dccbfca45ddb28bf0ee38c1c01efe53410/src/alm.jl#L55-L72">source</a></section></article><p>An instance of <code>DMap</code> (or <code>DAlm</code>) embeds, whithin the field <code>info</code>, a <a href="#HealpixMPI.GeomInfoMPI"><code>GeomInfoMPI</code></a> (or <a href="#HealpixMPI.AlmInfoMPI"><code>AlmInfoMPI</code></a>) object. These latter, in turn, contain all the necessairy information about:</p><ul><li>The whole map geometry (or the whole set of harmonic coefficients).</li><li>The composition of the <em>local</em> subset.</li><li>the MPI communicator.</li></ul><article class="docstring"><header><a class="docstring-binding" id="HealpixMPI.GeomInfoMPI" href="#HealpixMPI.GeomInfoMPI"><code>HealpixMPI.GeomInfoMPI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct GeomInfoMPI{I &lt;: Integer, T &lt;: Real}</code></pre><p>Information describing an MPI-distributed subset of a <code>HealpixMap</code>, contained in a <code>DMap</code>.</p><p>A <code>GeomInfoMPI</code> type contains:</p><ul><li><code>comm</code>: MPI communicator used.</li><li><code>nside</code>: NSIDE parameter of the whole map.</li><li><code>maxnr</code>: maximum number of rings in the subsets, over the tasks involved.</li><li><code>thetatot</code>: array of the colatitudes of the whole map ordered by task first and RR within each task</li><li><code>rings</code>: array of the ring indexes (w.r.t. the whole map) contained in the subset.</li><li><code>rstart</code>: array containing the 1-based index of the first pixel of each ring contained in the subset.</li><li><code>nphi</code>: array containing the number of pixels in every ring contained in the subset.</li><li><code>theta</code>: array of colatitudes (in radians) of the rings contained in the subset.</li><li><code>phi0</code>: array containing the values of the azimuth (in radians) of the first pixel in every ring.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/9d8251dccbfca45ddb28bf0ee38c1c01efe53410/src/map.jl#L2-L18">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="HealpixMPI.AlmInfoMPI" href="#HealpixMPI.AlmInfoMPI"><code>HealpixMPI.AlmInfoMPI</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">struct AlmInfoMPI{I &lt;: Integer}</code></pre><p>Information describing an MPI-distributed subset of <code>Alm</code>, contained in a <code>DAlm</code>.</p><p>An <code>AlmInfoMPI</code> type contains:</p><ul><li><code>comm</code>: MPI communicator used.</li><li><code>lmax</code>: the maximum value of <span>$ℓ$</span>.</li><li><code>mmax</code>: the maximum value of <span>$m$</span> on the whole <code>Alm</code> set.</li><li><code>maxnm</code>: maximum value (over tasks) of nm (number of m values).</li><li><code>mval</code>: array of values of <span>$m$</span> contained in the subset.</li><li><code>mstart</code>: array hypothetical indexes of the harmonic coefficient with ℓ=0, m. #FIXME: for now 0-based</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/9d8251dccbfca45ddb28bf0ee38c1c01efe53410/src/alm.jl#L16-L29">source</a></section></article><h2 id="Initializing-a-Distributed-type"><a class="docs-heading-anchor" href="#Initializing-a-Distributed-type">Initializing a Distributed type</a><a id="Initializing-a-Distributed-type-1"></a><a class="docs-heading-anchor-permalink" href="#Initializing-a-Distributed-type" title="Permalink"></a></h2><p>The recommended way to construct a local subset of a map or harmonic coefficients, is to start with an instance of <code>HealpixMap</code> (in <code>RingOrder</code>) or <code>Alm</code> on the root task, and call one of the apposite overloads of the standard <code>MPI.Scatter!</code> function, provided by HealpixMPI.jl. Such function would in fact save the user the job of constructing all the required ancillary information describing the data subset, doing so through efficient and tested methods.</p><article class="docstring"><header><a class="docstring-binding" id="MPI.Scatter!" href="#MPI.Scatter!"><code>MPI.Scatter!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Scatter!(in_alm::Alm{T,Array{T,1}}, out_d_alm::DAlm{T}, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {S&lt;:Strategy, T&lt;:Number, I&lt;:Integer}
Scatter!(in_alm::Nothing, out_d_alm::DAlm{T}, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {S&lt;:Strategy, T&lt;:Number, I&lt;:Integer}
Scatter!(in_alm, out_d_alm::DAlm{S,T,I}, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {S&lt;:Strategy, T&lt;:Number, I&lt;:Integer}</code></pre><p>Distributes the <code>Alm</code> object passed in input on the <code>root</code> task overwriting the <code>DAlm</code> objects passed on each task, according to the specified strategy (by default &quot;:RR&quot; for Round Robin).</p><p>As in the standard MPI function, the <code>in_alm</code> in input can be <code>nothing</code> on non-root tasks, since it will be ignored anyway.</p><p>If the keyword <code>clear</code> is set to <code>true</code> it frees the memory of each task from the (potentially bulky) <code>Alm</code> object.</p><p><strong>Arguments:</strong></p><ul><li><code>in_alm::Alm{T,Array{T,1}}</code>: <code>Alm</code> object to distribute over the MPI tasks.</li><li><code>out_d_alm::DAlm{T}</code>: output <code>DAlm</code> object.</li></ul><p><strong>Keywords:</strong></p><ul><li><code>root::Integer</code>: rank of the task to be considered as &quot;root&quot;, it is 0 by default.</li><li><code>clear::Bool</code>: if true deletes the input <code>Alm</code> after having performed the &quot;scattering&quot;.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/9d8251dccbfca45ddb28bf0ee38c1c01efe53410/src/alm.jl#L174-L195">source</a></section><section><div><pre><code class="nohighlight hljs">Scatter!(in_map::HealpixMap{T,RingOrder,Array{T,1}}, out_d_map::DMap{S,T,I}, strategy::Symbol, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {T &lt;: Number, I &lt;: Integer}
Scatter!(nothing, out_d_map::DMap{S,T,I}, strategy::Symbol, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {T &lt;: Number, I &lt;: Integer}</code></pre><p>Distributes the <code>HealpixMap</code> object passed in input on the <code>root</code> task overwriting the <code>DMap</code> objects passed on each task, according to the specified strategy (by default &quot;:RR&quot; for Round Robin).</p><p>As in the standard MPI function, the <code>in_map</code> in input can be <code>nothing</code> on non-root tasks, since it will be ignored anyway.</p><p>If the keyword <code>clear</code> is set to <code>true</code> it frees the memory of each task from the (potentially bulky) <code>HealpixMap</code> object.</p><p><strong>Arguments:</strong></p><ul><li><code>in_map::HealpixMap{T,RingOrder,Array{T,1}}</code>: <code>HealpixMap</code> object to distribute over the MPI tasks.</li><li><code>out_d_map::DMap{S,T,I}</code>: output <code>DMap</code> object.</li></ul><p><strong>Keywords:</strong></p><ul><li><code>root::Integer</code>: rank of the task to be considered as &quot;root&quot;, it is 0 by default.</li><li><code>clear::Bool</code>: if true deletes the input map after having performed the &quot;scattering&quot;.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/9d8251dccbfca45ddb28bf0ee38c1c01efe53410/src/map.jl#L190-L210">source</a></section></article><p>While distributing a set of harmonic coefficients means that each MPI task will host a <code>DAlm</code> object containing only the coefficients corresponding to some specific values of m, the distribution of a map is performed by rings. Each MPI task will then host a <code>DMap</code> object containing only the pixels composing some specified rings of the entire <code>HealpixMap</code>. Note that, for spherical harmonic transforms efficiency, it is recommended to assign pairs of rings with same latitude (i.e. symmetric w.r.t. the equator) to the same task, in order to preserve the geometric symmetry of the map.</p><p>The following example shows the standard way to initialize a <code>DAlm</code> object through a round robin strategy (see the paragraph <a href="https://leeobianchi.github.io/HealpixMPI.jl/dev/distribute/#Distributing-Strategy"><code>Distributing Strategy</code></a> for more details about this).</p><pre><code class="language-julia hljs">using HealpixMPI

MPI.Init()
comm = MPI.COMM_WORLD

alm = Alm(5, 5, randn(ComplexF64, numberOfAlms(5))) #inizialize random Healpix Alm
d_alm = DAlm{RR}() #inizialize empty DAlm to be filled according to RR strategy

MPI.Scatter!(alm, d_alm, comm) #fill d_alm</code></pre><h2 id="Gathering-data"><a class="docs-heading-anchor" href="#Gathering-data">Gathering data</a><a id="Gathering-data-1"></a><a class="docs-heading-anchor-permalink" href="#Gathering-data" title="Permalink"></a></h2><p>Analogously to <code>MPI.Scatter!</code>, HealpixMPI.jl also provides overloads of <code>MPI.Gather!</code> (and <code>MPI.Allgather!</code>). These latter allow to re-group subsets of map or alm into a <code>HealpixMap</code> or <code>Alm</code> only on the root task (or on every MPI task involved).</p><article class="docstring"><header><a class="docstring-binding" id="MPI.Gather!" href="#MPI.Gather!"><code>MPI.Gather!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Gather!(in_d_alm::DAlm{S,T,I}, out_alm::Alm{T,Array{T,1}}, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {S&lt;:Strategy, T&lt;:Number, I&lt;:Integer}
Gather!(in_d_alm::DAlm{S,T,I}, out_alm::Nothing, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {S&lt;:Strategy, T&lt;:Number, I&lt;:Integer}</code></pre><p>Gathers the <code>DAlm</code> objects passed on each task overwriting the <code>Alm</code> object passed in input on the <code>root</code> task according to the specified <code>strategy</code> (by default <code>:RR</code> for Round Robin). Note that the strategy must match the one used to &quot;scatter&quot; the a_lm.</p><p>As in the standard MPI function, the <code>out_alm</code> can be <code>nothing</code> on non-root tasks, since it will be ignored anyway.</p><p>If the keyword <code>clear</code> is set to <code>true</code> it frees the memory of each task from the (potentially bulky) <code>DAlm</code> object.</p><p><strong>Arguments:</strong></p><ul><li><code>in_d_alm::DAlm{T}</code>: <code>DAlm</code> object to gather from the MPI tasks.</li><li><code>out_d_alm::Alm{T,Array{T,1}}</code>: output <code>Alm</code> object.</li></ul><p><strong>Keywords:</strong></p><ul><li><code>strategy::Symbol</code>: Strategy to be used, by default <code>:RR</code> for &quot;Round Robin&quot;.</li><li><code>root::Integer</code>: rank of the task to be considered as &quot;root&quot;, it is 0 by default.</li><li><code>clear::Bool</code>: if true deletes the input <code>Alm</code> after having performed the &quot;scattering&quot;.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/9d8251dccbfca45ddb28bf0ee38c1c01efe53410/src/alm.jl#L320-L343">source</a></section><section><div><pre><code class="nohighlight hljs">Gather!(in_d_map::DMap{T, I}, out_map::HealpixMap{T,RingOrder,Array{T,1}}, strategy::Symbol, comm::MPI.Comm; root::Integer = 0, clear::Bool = false)
Gather!(in_d_map::DMap{T, I}, out_map::Nothing, strategy::Symbol, comm::MPI.Comm; root::Integer = 0, clear::Bool = false)</code></pre><p>Gathers the <code>DMap</code> objects passed on each task overwriting the <code>HealpixMap</code> object passed in input on the <code>root</code> task according to the specified <code>strategy</code> (by default <code>:RR</code> for Round Robin). Note that the strategy must match the one used to &quot;scatter&quot; the map.</p><p>As in the standard MPI function, the <code>out_map</code> can be <code>nothing</code> on non-root tasks, since it will be ignored anyway.</p><p>If the keyword <code>clear</code> is set to <code>true</code> it frees the memory of each task from the (potentially bulky) <code>DMap</code> object.</p><p><strong>Arguments:</strong></p><ul><li><code>in_d_map::DMap{T, I}</code>: <code>DMap</code> object to gather from the MPI tasks.</li><li><code>out_map::HealpixMap{T,RingOrder,Array{T,1}}</code>: output <code>Map</code> object.</li></ul><p><strong>Keywords:</strong></p><ul><li><code>strategy::Symbol</code>: Strategy to be used, by default <code>:RR</code> for &quot;Round Robin&quot;.</li><li><code>root::Integer</code>: rank of the task to be considered as &quot;root&quot;, it is 0 by default.</li><li><code>clear::Bool</code>: if true deletes the input <code>DMap</code> after having performed the &quot;scattering&quot;.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/9d8251dccbfca45ddb28bf0ee38c1c01efe53410/src/map.jl#L331-L354">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="MPI.Allgather!" href="#MPI.Allgather!"><code>MPI.Allgather!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Allgather!(in_d_alm::DAlm{S,T,I}, out_alm::Alm{T,Array{T,1}}; clear::Bool = false) where {S&lt;:Strategy, T&lt;:Number, I&lt;:Integer}</code></pre><p>Gathers the <code>DAlm</code> objects passed on each task overwriting the <code>Alm</code> object passed in input on the <code>root</code> task according to the specified <code>strategy</code> (by default <code>:RR</code> for Round Robin). Note that the strategy must match the one used to &quot;scatter&quot; the a_lm.</p><p>As in the standard MPI function, the <code>out_alm</code> can be <code>nothing</code> on non-root tasks, since it will be ignored anyway.</p><p>If the keyword <code>clear</code> is set to <code>true</code> it frees the memory of each task from the (potentially bulky) <code>DAlm</code> object.</p><p><strong>Arguments:</strong></p><ul><li><code>in_d_alm::DAlm{T}</code>: <code>DAlm</code> object to gather from the MPI tasks.</li><li><code>out_d_alm::Alm{T,Array{T,1}}</code>: output <code>Alm</code> object.</li></ul><p><strong>Keywords:</strong></p><ul><li><code>strategy::Symbol</code>: Strategy to be used, by default <code>:RR</code> for &quot;Round Robin&quot;.</li><li><code>clear::Bool</code>: if true deletes the input <code>Alm</code> after having performed the &quot;scattering&quot;.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/9d8251dccbfca45ddb28bf0ee38c1c01efe53410/src/alm.jl#L415-L436">source</a></section><section><div><pre><code class="nohighlight hljs">Allgather!(in_d_map::DMap{S,T,I}, out_map::HealpixMap{T,RingOrder,Array{T,1}}, strategy::Symbol, comm::MPI.Comm; clear::Bool = false) where {T &lt;: Number}</code></pre><p>Gathers the <code>DMap</code> objects passed on each task overwriting the <code>out_map</code> object passed in input on EVERY task according to the specified <code>strategy</code> (by default <code>:RR</code> for Round Robin). Note that the strategy must match the one used to &quot;scatter&quot; the map.</p><p>If the keyword <code>clear</code> is set to <code>true</code> it frees the memory of each task from the (potentially bulky) <code>DMap</code> object.</p><p><strong>Arguments:</strong></p><ul><li><code>in_d_map::DMap{S,T,I}</code>: <code>DMap</code> object to gather from the MPI tasks.</li><li><code>out_d_map::HealpixMap{T,RingOrder,Array{T,1}}</code>: output <code>HealpixMap</code> object to overwrite.</li></ul><p><strong>Keywords:</strong></p><ul><li><code>clear::Bool</code>: if true deletes the input <code>Alm</code> after having performed the &quot;scattering&quot;.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/LeeoBianchi/HealpixMPI.jl/blob/9d8251dccbfca45ddb28bf0ee38c1c01efe53410/src/map.jl#L424-L441">source</a></section></article><h2 id="Distributing-Strategy"><a class="docs-heading-anchor" href="#Distributing-Strategy">Distributing Strategy</a><a id="Distributing-Strategy-1"></a><a class="docs-heading-anchor-permalink" href="#Distributing-Strategy" title="Permalink"></a></h2><p>It is also worth mentioning that one could find many different strategies to distribute a set of data over multiple MPI tasks. So far, the only one implemented in HealpixMPI.jl, which should guarantee an adequate work balance between tasks, is the so-called &quot;round robin&quot; strategy: assuming <span>$N$</span> MPI tasks, the map is distributed such that task <span>$i$</span> hosts the map rings <span>$i$</span>, <span>$i + N$</span>, <span>$i + 2N$</span>, etc. (and their counterparts on the other hemisphere). Similarly, for the spherical harmonic coefficients, task <span>$i$</span> would hold all coefficients for <span>$m = i$</span>, <span>$i + N$</span>, <span>$i + 2 N$</span>, etc.</p><p>The strategy is intrinsically specified in a <code>DMap</code> or <code>DAlm</code> instance through an abstract type (e.g. <code>RR</code>), inherited from a super-type <code>Strategy</code>; in the same way as the pixel ordering is specified in a <code>HealpixMap</code> in Healpix.jl.</p><p>This kind of solution allows for two great features:</p><ul><li><p>An efficient and fast multiple-dispatch, allowing a function to recognize the distribution strategy used on data structure without the usage of any <code>if</code> statement.</p></li><li><p>Allows to add other distributing strategies if needed for future developments by simply adding an inherited type in the source file <code>strategy.jl</code> with a single line of code:</p></li></ul><pre><code class="language-julia hljs">abstract type NewStrat&lt;:Strategy end</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Introduction</a><a class="docs-footer-nextpage" href="../sht/">Spherical Harmonics »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Wednesday 22 March 2023 15:56">Wednesday 22 March 2023</span>. Using Julia version 1.6.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
