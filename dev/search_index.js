var documenterSearchIndex = {"docs":
[{"location":"misc/","page":"Miscellanea","title":"Miscellanea","text":"DocTestSetup = quote\n    using HealpixMPI\nend","category":"page"},{"location":"misc/","page":"Miscellanea","title":"Miscellanea","text":"HealpixMPI.jl provides MPI-parallel versions, through overloads, of most of the other spherical-harmonic related functions of Healpix.jl. Refer to Healpix.jl documentation for their description.","category":"page"},{"location":"misc/#Algebraic-operations-in-harmonic-space","page":"Miscellanea","title":"Algebraic operations in harmonic space","text":"","category":"section"},{"location":"misc/","page":"Miscellanea","title":"Miscellanea","text":"HealpixMPI.jl provides overloads of the Base functions +, -, *, /, as well as LinearAlgebra.dot (which embeds and MPI.Allreduce call), allowing to carry out these fundamental operations element-wise in harmonic space directly.","category":"page"},{"location":"misc/","page":"Miscellanea","title":"Miscellanea","text":"almxfl\nalmxfl!\nBase.:+\nBase.:-\nBase.:*\nBase.:/\nLinearAlgebra.dot","category":"page"},{"location":"misc/#Healpix.almxfl","page":"Miscellanea","title":"Healpix.almxfl","text":"almxfl(alm::DAlm{S,T,I}, fl::AA) where {S<:Strategy, T<:Number, I<:Integer, AA<:AbstractArray{T,1}}\n\nMultiply a subset of aℓm in the form of DAlm by a vector bℓ representing an ℓ-dependent function, without changing the a_ℓm passed in input.\n\nArguments\n\nalm::DAlm{S,T,I}: The array representing the spherical harmonics coefficients\nfl::AbstractVector{T}: The array giving the factor fℓ by which to multiply aℓm\n\nReturns\n\nAlm{S,T}: The result of aℓm * fℓ.\n\n\n\n\n\n","category":"function"},{"location":"misc/#Healpix.almxfl!","page":"Miscellanea","title":"Healpix.almxfl!","text":"almxfl!(alm::DAlm{S,T,I}, fl::AA) where {S<:Strategy, T<:Number, I<:Integer, AA<:AbstractArray{T,1}}\n\nMultiply IN-PLACE a subset of a_ℓm in the form of DAlm by a vector fl representing an ℓ-dependent function.\n\nArguments\n\nalms::DAlm{S,T,I}: The subset of spherical harmonics coefficients\nfl::AbstractVector{T}: The array giving the factor fℓ by which to multiply aℓm\n\n\n\n\n\n","category":"function"},{"location":"misc/#Base.:+","page":"Miscellanea","title":"Base.:+","text":"+(alm₁::DAlm{S,T,I}, alm₂::DAlm{S,T,I}) where {S<:Strategy, T<:Number, I<:Integer}\n\nPerform the element-wise SUM of two DAlm objects in a_ℓm space. A new DAlm object is returned.\n\n\n\n\n\n","category":"function"},{"location":"misc/#Base.:-","page":"Miscellanea","title":"Base.:-","text":"-(alm₁::DAlm{S,T,I}, alm₂::DAlm{S,T,I}) where {S<:Strategy, T<:Number, I<:Integer}\n\nPerform the element-wise SUBTRACTION of two DAlm objects in a_ℓm space. A new DAlm object is returned.\n\n\n\n\n\n","category":"function"},{"location":"misc/#Base.:*","page":"Miscellanea","title":"Base.:*","text":"*(alm::DAlm{S,T,I}, fl::AA) where {S<:Strategy, T<:Number, I<:Integer, AA<:AbstractArray{T,1}}\n*(fl::AA, alm::DAlm{S,T,I}) where {S<:Strategy, T<:Number, I<:Integer, AA<:AbstractArray{T,1}}\n\nPerform the MULTIPLICATION of a DAlm object by a function of ℓ in a_ℓm space. Note: this consists in a shortcut of almxfl, therefore a new DAlm object is returned.\n\n\n\n\n\n*(alm₁::DAlm{S,T,I}, alm₂::DAlm{S,T,I}) where {S<:Strategy, T<:Number, I<:Integer}\n*(alm₁::DAlm{S,T,I}, c::Number) where {S<:Strategy, T<:Number, I<:Integer}\n*(c::Number, alm₁::DAlm{S,T,I}) where {S<:Strategy, T<:Number, I<:Integer}\n\nPerform the element-wise MULTIPLICATION of two DAlm objects or of a DAlm by a constant in a_ℓm space. A new DAlm object is returned.\n\n\n\n\n\n","category":"function"},{"location":"misc/#Base.:/","page":"Miscellanea","title":"Base.:/","text":"/(alm::DAlm{S,T,I}, fl::AA) where {S<:Strategy, T<:Number, I<:Integer, AA<:AbstractArray{T,1}}\n\nPerform an element-wise DIVISION by a function of ℓ in a_ℓm space. Note: this consists in a shortcut of almxfl, therefore a new DAlm object is returned.\n\n\n\n\n\n/(alm₁::DAlm{S,T,I}, alm₂::DAlm{S,T,I}) where {S<:Strategy, T<:Number, I<:Integer}\n/(alm₁::DAlm{S,T,I}, c::Number) where {S<:Strategy, T<:Number, I<:Integer}\n\nPerform the element-wise DIVISION of two DAlm objects or of a DAlm by a constant in a_ℓm space. A new DAlm object is returned.\n\n\n\n\n\n","category":"function"},{"location":"misc/#LinearAlgebra.dot","page":"Miscellanea","title":"LinearAlgebra.dot","text":"dot(alm₁::DAlm{S,T,I}, alm₂::DAlm{S,T,I}) where {S<:Strategy, T<:Number, I<:Integer} -> Number\n\nMPI-parallel dot product between two DAlm object of matching size.\n\n\n\n\n\n","category":"function"},{"location":"misc/#Power-spectrum","page":"Miscellanea","title":"Power spectrum","text":"","category":"section"},{"location":"misc/","page":"Miscellanea","title":"Miscellanea","text":"Power spectrum components C_ell are encoded as Vector{T}. HealpixMPI.jl implements overloads of Healpix.jl functions to compute a power spectrum from a set of DAlm (alm2cl) and to generate a set of DAlm from a power spectrum (synalm!).","category":"page"},{"location":"misc/","page":"Miscellanea","title":"Miscellanea","text":"alm2cl\nsynalm!","category":"page"},{"location":"misc/#Healpix.alm2cl","page":"Miscellanea","title":"Healpix.alm2cl","text":"alm2cl(alm₁::DAlm{S,T,I}, alm₂::DAlm{S,T,I}) where {S<:Strategy, T<:Number, I<:Integer} -> Vector{T}\nalm2cl(alm::DAlm{S,T,I}) where {S<:Strategy, T<:Number, I<:Integer} -> Vector{T}\n\nCompute the power spectrum C_ell on each MPI task from the spherical harmonic coefficients of one or two fields, distributed as DAlm.\n\n\n\n\n\n","category":"function"},{"location":"misc/#Healpix.synalm!","page":"Miscellanea","title":"Healpix.synalm!","text":"synalm!(cl::Vector{T}, alm::DAlm{S,N,I}, rng::AbstractRNG) where {S<:Strategy, T<:Real, N<:Number, I<:Integer}\nsynalm!(cl::Vector{T}, alm::DAlm{S,N,I}) where {S<:Strategy, T<:Real, N<:Number, I<:Integer}\n\nGenerate a set of DAlm from a given power spectra array cl. The output is written into the Alm object passed in input. An RNG can be specified, otherwise it's defaulted to Random.GLOBAL_RNG.\n\n\n\n\n\n","category":"function"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"DocTestSetup = quote\n    using HealpixMPI\nend","category":"page"},{"location":"distribute/#Distributed-Classes","page":"Distributed Classes","title":"Distributed Classes","text":"","category":"section"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"As mentioned in the introduction, HealpixMPI has the main purpose of providing an MPI parallelization of the main functionalities of Healpix.jl, distributing maps and harmonic coefficients over the MPI tasks efficiently. This is made possible by the implementation of two data types: DMap and DAlm, mirroring HealpixMap and Alm types of Healpix.jl respectively, and containing a well-defined subset of a map or harmonic coefficients, to be constructed on each MPI task.","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"DMap\nDAlm","category":"page"},{"location":"distribute/#HealpixMPI.DMap","page":"Distributed Classes","title":"HealpixMPI.DMap","text":"struct DMap{T<:Number, I<:Integer}\n\nA subset of a Healpix map, containing only certain rings (as specified in the info field). The type T is used for the value of the pixels in a map, it must be a Number (usually float).\n\nA DMap type contains the following fields:\n\npixels::Vector{T}: array of pixels composing the subset.\ninfo::GeomInfo: a GeomInfo object describing the HealpixMap subset.\n\nThe GeomInfoMPI contained in info must match exactly the characteristic of the Map subset, this is already automatically constructed when MPI.Scatter! is called, reason why this method for initializing a DMap is reccomended.\n\n\n\n\n\n","category":"type"},{"location":"distribute/#HealpixMPI.DAlm","page":"Distributed Classes","title":"HealpixMPI.DAlm","text":"struct DAlm{S<:Strategy, T<:Number, I<:Integer}\n\nAn MPI-distributed subset of harmonic coefficients a_ℓm, referring only to certain values of m.\n\nThe type T is used for the value of each harmonic coefficient, and it must be a Number (one should however only use complex types for this).\n\nA SubAlm type contains the following fields:\n\nalm: the array of harmonic coefficients\ninfo: an AlmInfo object describing the alm subset.\n\nThe AlmInfo contained in info must match exactly the characteristic of the Alm subset, this can be constructed through the function make_general_alm_info, for instance.\n\n\n\n\n\n","category":"type"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"An instance of DMap (or DAlm) embeds, whithin the field info, a GeomInfoMPI (or AlmInfoMPI) object. These latter, in turn, contain all the necessairy information about:","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"The whole map geometry (or the whole set of harmonic coefficients).\nThe composition of the local subset.\nthe MPI communicator.","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"GeomInfoMPI\nAlmInfoMPI","category":"page"},{"location":"distribute/#HealpixMPI.GeomInfoMPI","page":"Distributed Classes","title":"HealpixMPI.GeomInfoMPI","text":"struct GeomInfoMPI{I <: Integer, T <: Real}\n\nInformation describing an MPI-distributed subset of a HealpixMap, contained in a DMap.\n\nA GeomInfoMPI type contains:\n\ncomm: MPI communicator used.\nnside: NSIDE parameter of the whole map.\nmaxnr: maximum number of rings in the subsets, over the tasks involved.\nthetatot: array of the colatitudes of the whole map ordered by task first and RR within each task\nrings: array of the ring indexes (w.r.t. the whole map) contained in the subset.\nrstart: array containing the 1-based index of the first pixel of each ring contained in the subset.\nnphi: array containing the number of pixels in every ring contained in the subset.\ntheta: array of colatitudes (in radians) of the rings contained in the subset.\nphi0: array containing the values of the azimuth (in radians) of the first pixel in every ring.\n\n\n\n\n\n","category":"type"},{"location":"distribute/#HealpixMPI.AlmInfoMPI","page":"Distributed Classes","title":"HealpixMPI.AlmInfoMPI","text":"struct AlmInfoMPI{I <: Integer}\n\nInformation describing an MPI-distributed subset of Alm, contained in a DAlm.\n\nAn AlmInfoMPI type contains:\n\ncomm: MPI communicator used.\nlmax: the maximum value of ℓ.\nmmax: the maximum value of m on the whole Alm set.\nmaxnm: maximum value (over tasks) of nm (number of m values).\nmval: array of values of m contained in the subset.\nmstart: array hypothetical indexes of the harmonic coefficient with ℓ=0, m. #FIXME: for now 0-based\n\n\n\n\n\n","category":"type"},{"location":"distribute/#Initializing-a-Distributed-type","page":"Distributed Classes","title":"Initializing a Distributed type","text":"","category":"section"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"The recommended way to construct a local subset of a map or harmonic coefficients, is to start with an instance of HealpixMap (in RingOrder) or Alm on the root task, and call one of the apposite overloads of the standard MPI.Scatter! function, provided by HealpixMPI.jl. Such function would in fact save the user the job of constructing all the required ancillary information describing the data subset, doing so through efficient and tested methods.","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"MPI.Scatter!","category":"page"},{"location":"distribute/#MPI.Scatter!","page":"Distributed Classes","title":"MPI.Scatter!","text":"Scatter!(in_alm::Alm{T,Array{T,1}}, out_d_alm::DAlm{T}, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {S<:Strategy, T<:Number, I<:Integer}\nScatter!(in_alm::Nothing, out_d_alm::DAlm{T}, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {S<:Strategy, T<:Number, I<:Integer}\nScatter!(in_alm, out_d_alm::DAlm{S,T,I}, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {S<:Strategy, T<:Number, I<:Integer}\n\nDistributes the Alm object passed in input on the root task overwriting the DAlm objects passed on each task, according to the specified strategy (by default \":RR\" for Round Robin).\n\nAs in the standard MPI function, the in_alm in input can be nothing on non-root tasks, since it will be ignored anyway.\n\nIf the keyword clear is set to true it frees the memory of each task from the (potentially bulky) Alm object.\n\nArguments:\n\nin_alm::Alm{T,Array{T,1}}: Alm object to distribute over the MPI tasks.\nout_d_alm::DAlm{T}: output DAlm object.\n\nKeywords:\n\nroot::Integer: rank of the task to be considered as \"root\", it is 0 by default.\nclear::Bool: if true deletes the input Alm after having performed the \"scattering\".\n\n\n\n\n\nScatter!(in_map::HealpixMap{T,RingOrder,Array{T,1}}, out_d_map::DMap{S,T,I}, strategy::Symbol, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {T <: Number, I <: Integer}\nScatter!(nothing, out_d_map::DMap{S,T,I}, strategy::Symbol, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {T <: Number, I <: Integer}\n\nDistributes the HealpixMap object passed in input on the root task overwriting the DMap objects passed on each task, according to the specified strategy (by default \":RR\" for Round Robin).\n\nAs in the standard MPI function, the in_map in input can be nothing on non-root tasks, since it will be ignored anyway.\n\nIf the keyword clear is set to true it frees the memory of each task from the (potentially bulky) HealpixMap object.\n\nArguments:\n\nin_map::HealpixMap{T,RingOrder,Array{T,1}}: HealpixMap object to distribute over the MPI tasks.\nout_d_map::DMap{S,T,I}: output DMap object.\n\nKeywords:\n\nroot::Integer: rank of the task to be considered as \"root\", it is 0 by default.\nclear::Bool: if true deletes the input map after having performed the \"scattering\".\n\n\n\n\n\n","category":"function"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"While distributing a set of harmonic coefficients means that each MPI task will host a DAlm object containing only the coefficients corresponding to some specific values of m, the distribution of a map is performed by rings. Each MPI task will then host a DMap object containing only the pixels composing some specified rings of the entire HealpixMap. Note that, for spherical harmonic transforms efficiency, it is recommended to assign pairs of rings with same latitude (i.e. symmetric w.r.t. the equator) to the same task, in order to preserve the geometric symmetry of the map.","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"The following example shows the standard way to initialize a DAlm object through a round robin strategy (see the paragraph Distributing Strategy for more details about this).","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"using HealpixMPI\n\nMPI.Init()\ncomm = MPI.COMM_WORLD\n\nalm = Alm(5, 5, randn(ComplexF64, numberOfAlms(5))) #inizialize random Healpix Alm\nd_alm = DAlm{RR}() #inizialize empty DAlm to be filled according to RR strategy\n\nMPI.Scatter!(alm, d_alm, comm) #fill d_alm","category":"page"},{"location":"distribute/#Gathering-data","page":"Distributed Classes","title":"Gathering data","text":"","category":"section"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"Analogously to MPI.Scatter!, HealpixMPI.jl also provides overloads of MPI.Gather! (and MPI.Allgather!). These latter allow to re-group subsets of map or alm into a HealpixMap or Alm only on the root task (or on every MPI task involved).","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"MPI.Gather!\nMPI.Allgather!","category":"page"},{"location":"distribute/#MPI.Gather!","page":"Distributed Classes","title":"MPI.Gather!","text":"Gather!(in_d_alm::DAlm{S,T,I}, out_alm::Alm{T,Array{T,1}}, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {S<:Strategy, T<:Number, I<:Integer}\nGather!(in_d_alm::DAlm{S,T,I}, out_alm::Nothing, comm::MPI.Comm; root::Integer = 0, clear::Bool = false) where {S<:Strategy, T<:Number, I<:Integer}\n\nGathers the DAlm objects passed on each task overwriting the Alm object passed in input on the root task according to the specified strategy (by default :RR for Round Robin). Note that the strategy must match the one used to \"scatter\" the a_lm.\n\nAs in the standard MPI function, the out_alm can be nothing on non-root tasks, since it will be ignored anyway.\n\nIf the keyword clear is set to true it frees the memory of each task from the (potentially bulky) DAlm object.\n\nArguments:\n\nin_d_alm::DAlm{T}: DAlm object to gather from the MPI tasks.\nout_d_alm::Alm{T,Array{T,1}}: output Alm object.\n\nKeywords:\n\nstrategy::Symbol: Strategy to be used, by default :RR for \"Round Robin\".\nroot::Integer: rank of the task to be considered as \"root\", it is 0 by default.\nclear::Bool: if true deletes the input Alm after having performed the \"scattering\".\n\n\n\n\n\nGather!(in_d_map::DMap{T, I}, out_map::HealpixMap{T,RingOrder,Array{T,1}}, strategy::Symbol, comm::MPI.Comm; root::Integer = 0, clear::Bool = false)\nGather!(in_d_map::DMap{T, I}, out_map::Nothing, strategy::Symbol, comm::MPI.Comm; root::Integer = 0, clear::Bool = false)\n\nGathers the DMap objects passed on each task overwriting the HealpixMap object passed in input on the root task according to the specified strategy (by default :RR for Round Robin). Note that the strategy must match the one used to \"scatter\" the map.\n\nAs in the standard MPI function, the out_map can be nothing on non-root tasks, since it will be ignored anyway.\n\nIf the keyword clear is set to true it frees the memory of each task from the (potentially bulky) DMap object.\n\nArguments:\n\nin_d_map::DMap{T, I}: DMap object to gather from the MPI tasks.\nout_map::HealpixMap{T,RingOrder,Array{T,1}}: output Map object.\n\nKeywords:\n\nstrategy::Symbol: Strategy to be used, by default :RR for \"Round Robin\".\nroot::Integer: rank of the task to be considered as \"root\", it is 0 by default.\nclear::Bool: if true deletes the input DMap after having performed the \"scattering\".\n\n\n\n\n\n","category":"function"},{"location":"distribute/#MPI.Allgather!","page":"Distributed Classes","title":"MPI.Allgather!","text":"Allgather!(in_d_alm::DAlm{S,T,I}, out_alm::Alm{T,Array{T,1}}; clear::Bool = false) where {S<:Strategy, T<:Number, I<:Integer}\n\nGathers the DAlm objects passed on each task overwriting the Alm object passed in input on the root task according to the specified strategy (by default :RR for Round Robin). Note that the strategy must match the one used to \"scatter\" the a_lm.\n\nAs in the standard MPI function, the out_alm can be nothing on non-root tasks, since it will be ignored anyway.\n\nIf the keyword clear is set to true it frees the memory of each task from the (potentially bulky) DAlm object.\n\nArguments:\n\nin_d_alm::DAlm{T}: DAlm object to gather from the MPI tasks.\nout_d_alm::Alm{T,Array{T,1}}: output Alm object.\n\nKeywords:\n\nstrategy::Symbol: Strategy to be used, by default :RR for \"Round Robin\".\nclear::Bool: if true deletes the input Alm after having performed the \"scattering\".\n\n\n\n\n\nAllgather!(in_d_map::DMap{S,T,I}, out_map::HealpixMap{T,RingOrder,Array{T,1}}, strategy::Symbol, comm::MPI.Comm; clear::Bool = false) where {T <: Number}\n\nGathers the DMap objects passed on each task overwriting the out_map object passed in input on EVERY task according to the specified strategy (by default :RR for Round Robin). Note that the strategy must match the one used to \"scatter\" the map.\n\nIf the keyword clear is set to true it frees the memory of each task from the (potentially bulky) DMap object.\n\nArguments:\n\nin_d_map::DMap{S,T,I}: DMap object to gather from the MPI tasks.\nout_d_map::HealpixMap{T,RingOrder,Array{T,1}}: output HealpixMap object to overwrite.\n\nKeywords:\n\nclear::Bool: if true deletes the input Alm after having performed the \"scattering\".\n\n\n\n\n\n","category":"function"},{"location":"distribute/#Distributing-Strategy","page":"Distributed Classes","title":"Distributing Strategy","text":"","category":"section"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"It is also worth mentioning that one could find many different strategies to distribute a set of data over multiple MPI tasks. So far, the only one implemented in HealpixMPI.jl, which should guarantee an adequate work balance between tasks, is the so-called \"round robin\" strategy: assuming N MPI tasks, the map is distributed such that task i hosts the map rings i, i + N, i + 2N, etc. (and their counterparts on the other hemisphere). Similarly, for the spherical harmonic coefficients, task i would hold all coefficients for m = i, i + N, i + 2 N, etc.","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"The strategy is intrinsically specified in a DMap or DAlm instance through an abstract type (e.g. RR), inherited from a super-type Strategy; in the same way as the pixel ordering is specified in a HealpixMap in Healpix.jl.","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"This kind of solution allows for two great features:","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"An efficient and fast multiple-dispatch, allowing a function to recognize the distribution strategy used on data structure without the usage of any if statement.\nAllows to add other distributing strategies if needed for future developments by simply adding an inherited type in the source file strategy.jl with a single line of code:","category":"page"},{"location":"distribute/","page":"Distributed Classes","title":"Distributed Classes","text":"abstract type NewStrat<:Strategy end","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"DocTestSetup = quote\n    using HealpixMPI\nend","category":"page"},{"location":"#HealpixMPI.jl:-an-MPI-parallel-implementation-of-the-HEALPix-tessellation-scheme-in-Julia","page":"Introduction","title":"HealpixMPI.jl: an MPI-parallel implementation of the HEALPix tessellation scheme in Julia","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Welcome to HealpixMPI.jl, an MPI-parallel implementation of Healpix.jl, an Healpix spherical tessellation scheme written entirely in Julia.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"This package constitutes a natural extension of the package Healpix.jl, providing an MPI integration of the main functionalities, allowing for high-performances and better scaling on high resolutions.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"More specifically, three main features can be highlighted:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"High-performance spherical harmonic transforms. This library relies on ducc's state-of-the-art algorithms for performing fast and efficient SHTs.\nMassively parallelization. The simultaneous usage of modern C++ multithreading (provided by ducc, for shared-memory parallelization) and MPI (for distributed-memory parallelization) allows the code to be run in parallel on a large number of nodes. The code has currently been tested and benchmarked with performance improvements up to 1024 cores. Note that this would be in practice impossible to achieve without the usage of MPI, which allows to distribute maps and harmonic coefficients over different computing nodes, since generally such a high number of computing cores is never available on the same machine.\nCross-platform support. This package maintains the same multi-platform compatibility of Healpix.jl, thanks to the cross-platform support of MPI.jl, Ducc0.jl (ducc's wrapper package providing Julia interface) and Julia itself.","category":"page"},{"location":"#Documentation","page":"Introduction","title":"Documentation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"The documentation was built using Documenter.jl.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"using Dates # hide\nprintln(\"Documentation built on $(now()) using Julia $(VERSION).\") # hide","category":"page"},{"location":"#Index","page":"Introduction","title":"Index","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"sht/","page":"Spherical Harmonics","title":"Spherical Harmonics","text":"DocTestSetup = quote\n    using HealpixMPI\nend","category":"page"},{"location":"sht/#Spherical-Harmonic-Transforms","page":"Spherical Harmonics","title":"Spherical Harmonic Transforms","text":"","category":"section"},{"location":"sht/","page":"Spherical Harmonics","title":"Spherical Harmonics","text":"The main target of HealpixMPI.jl, in terms of run time speed up, are the SHTs, which often represent the \"bottle neck\" of simulation codes or data analysis pipelines.","category":"page"},{"location":"sht/","page":"Spherical Harmonics","title":"Spherical Harmonics","text":"As mentioned in the introduction, HealpixMPI.jl relies on ducc's state-of-the-art algorithms for performing the spherical harmonic transforms. In particular, its C++ functions are exploited for the computation of Legandre coefficients from alms and maps and vice versa, while the transposition of such coefficients between MPI tasks is entirely coded in Julia.","category":"page"},{"location":"sht/","page":"Spherical Harmonics","title":"Spherical Harmonics","text":"HealpixMPI.jl implements only the two exact spherical harmonic operators alm2map! (synthesis) and adjoint_alm2map! (adjoint synthesis), leaving to the user the task to implement the two corresponding inverse approximate operators, which can be done through many different approaches, from pixel and ring weights to conjugate gradient solver. Both alm2map! and adjoint_alm2map! are implemented as overloads of the Healpix.jl's functions.","category":"page"},{"location":"sht/#From-Alm-to-Map:-synthesis-operator","page":"Spherical Harmonics","title":"From Alm to Map: synthesis operator","text":"","category":"section"},{"location":"sht/","page":"Spherical Harmonics","title":"Spherical Harmonics","text":"The synthesis SHT (alm2map) is used to compute a map from a set of a_ell m coefficients. It is generally represented by the matrix operator mathrmY which is defined through an exact summation as f(theta phi) = mathrmY  a_ell m quad textwhere quad f(theta phi) = sum_ell=0^infty sum_m=-ell^ell a_ell m Y_ell m (theta phi)","category":"page"},{"location":"sht/","page":"Spherical Harmonics","title":"Spherical Harmonics","text":"alm2map!","category":"page"},{"location":"sht/#Healpix.alm2map!","page":"Spherical Harmonics","title":"Healpix.alm2map!","text":"alm2map!(d_alm::DAlm{S,N,I}, d_map::DMap{S,T,I}, aux_in_leg::StridedArray{Complex{T},3}, aux_out_leg::StridedArray{Complex{T},3}; nthreads::Integer = 0) where {S<:Strategy, N<:Number, T<:Real, I<:Integer}\nalm2map!(d_alm::DAlm{S,N,I}, d_map::DMap{S,T,I}; nthreads::Integer = 0) where {S<:Strategy, N<:Number, T<:Real, I<:Integer}\n\nThis function performs an MPI-parallel spherical harmonic transform, computing a distributed map from a set of DAlm and places the results in the passed d_map object.\n\nIt must be called simultaneously on all the MPI tasks containing the subsets which form exactly the whole map and alm.\n\nIt is possible to pass two auxiliary arrays where the Legandre coefficients will be stored during the transform, this avoids allocating extra memory and improves efficiency.\n\nArguments:\n\nd_alm::DAlm{S,N,I}: the MPI-distributed spherical harmonic coefficients to transform.\nd_map::DMap{S,T,I}: the MPI-distributed map that will contain the result.\n\nOptionals:\n\naux_in_leg::StridedArray{Complex{T},3}: (localnm, totnring, 1) auxiliary matrix for alm-side Legandre coefficients.\naux_out_leg::StridedArray{Complex{T},3}: (totnm, localnring, 1) auxiliary matrix for map-side Legandre coefficients.\n\nKeywords\n\nnthreads::Integer = 0: the number of threads to use for the computation if 0, use as many threads as there are hardware threads available on the system.\n\n\n\n\n\n","category":"function"},{"location":"sht/#From-Map-to-Alm:-adjoint-synthesis-operator","page":"Spherical Harmonics","title":"From Map to Alm: adjoint synthesis operator","text":"","category":"section"},{"location":"sht/","page":"Spherical Harmonics","title":"Spherical Harmonics","text":"The adjoint of the synthesis operator brings us from the map space to the harmonic space, as it is represented by the transpose mathrmY^mathrmT. Which is defined through: mathrmY^mathrmT f(theta phi) equiv sum_i = 1^N_mathrmpix Y^*_ell m i  f_i","category":"page"},{"location":"sht/","page":"Spherical Harmonics","title":"Spherical Harmonics","text":"Note that this does not give directly the a_ell m coefficients, i.e.,   mathrmY^mathrmT mathrmY neq mathbf1 In fact, mathrmY^-1 simeq mathrmW mathrmY^mathrmT Where mathrmW is a diagonal matrix whose non-zero elements are approximately constant and equal to 4 pi  N_mathrmpix, depending on the map pixelization. mathrmY^-1 is in fact an integral operator which must be approximated when implemented numerically.","category":"page"},{"location":"sht/","page":"Spherical Harmonics","title":"Spherical Harmonics","text":"adjoint_alm2map!","category":"page"},{"location":"sht/#Healpix.adjoint_alm2map!","page":"Spherical Harmonics","title":"Healpix.adjoint_alm2map!","text":"adjoint_alm2map!(d_map::DMap{S,T,I}, d_alm::DAlm{S,N,I}, aux_in_leg::StridedArray{Complex{T},3}, aux_out_leg::StridedArray{Complex{T},3}; nthreads = 0) where {S<:Strategy, N<:Number, T<:Real, I<:Integer}\nadjoint_alm2map!(d_map::DMap{S,T,I}, d_alm::DAlm{S,N,I}; nthreads::Integer = 0) where {S<:Strategy, N<:Number, T<:Real, I<:Integer}\n\nThis function performs an MPI-parallel spherical harmonic transform Yᵀ on the distributed map and places the results in the passed d_alm object.\n\nIt must be called simultaneously on all the MPI tasks containing the subsets which form exactly the whole map and alm.\n\nIt is possible to pass two auxiliary arrays where the Legandre coefficients will be stored during the transform, this avoids allocating extra memory and improves efficiency.\n\nArguments:\n\nd_map::DMap{S,T,I}: the distributed map that must be decomposed in spherical harmonics.\nalm::Alm{ComplexF64, Array{ComplexF64, 1}}: the spherical harmonic coefficients to be written to.\n\nOptionals:\n\naux_in_leg::StridedArray{Complex{T},3}: (localnm, totnring, 1) auxiliary matrix for map-side Legandre coefficients.\naux_out_leg::StridedArray{Complex{T},3}: (totnm, localnring, 1) auxiliary matrix for alm-side Legandre coefficients.\n\nKeywords\n\nnthreads::Integer = 0: the number of threads to use for the computation if 0, use as many threads as there are hardware threads available on the system.\n\n\n\n\n\n","category":"function"}]
}
